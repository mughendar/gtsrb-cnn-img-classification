{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd934560-71f8-41b6-ba2b-551594742332",
   "metadata": {},
   "source": [
    "***AIM***\n",
    "The German Traffic Sign Benchmark is a multi-class, single-image classification challenge held at the International Joint Conference on Neural Networks (IJCNN) 2011. We cordially invite researchers from relevant fields to participate: The competition is designed to allow for participation without special domain knowledge. Our benchmark has the following properties:\n",
    "\n",
    "Single-image, multi-class classification problem\n",
    "More than 40 classes\n",
    "More than 50,000 images in total\n",
    "Large, lifelike database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114a64e-e340-46a2-a241-fac7f6e90a58",
   "metadata": {},
   "source": [
    "***importing important libraries for model implementation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06553971-50e5-4dbd-90a6-c855f6ad6ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\Admin\\\\python pro\\\\gtrsb cnn')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Conv2D,MaxPool2D,Dense,Flatten,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cad98f1-f109-4821-a57b-a0db47537c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b062a3e5-3b01-4baf-a75f-61e4802c85cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d17e1-485c-4284-ab62-30c81d9c5085",
   "metadata": {},
   "source": [
    "***storing data and labels separatley***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed8f181c-53b3-4b75-952e-a45da4480573",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "labels=[]\n",
    "classes=43\n",
    "cur_path=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3f3996-d8c5-43dd-86fc-844359775d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Admin\\\\python pro\\\\gtrsb cnn'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7793fd1-6f7d-434f-99da-bce4ce628b3e",
   "metadata": {},
   "source": [
    "***preprocessing the data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "995648d7-1c13-43b3-aae2-7b997b83214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(classes):\n",
    "    path=os.path.join(cur_path,'train',str(i))\n",
    "    images=os.listdir(path)\n",
    "    for a in images:\n",
    "        try:\n",
    "            image=Image.open(path+'\\\\'+a)\n",
    "            image=image.resize((30,30))\n",
    "            image=np.array(image)\n",
    "            data.append(image)\n",
    "            labels.append(i)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b2c31-6bf5-41e6-b329-2e3daa069830",
   "metadata": {},
   "source": [
    "***converting list into array***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc34c74d-384f-430a-b8b1-71bbbd2521b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array(data)\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd4dd3de-3e36-46e5-9bac-488784636a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39209, 30, 30, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31c35594-08ed-4a0c-afad-7a176779139e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39209,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f79b122-e221-4307-bd28-230c2046efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(data,labels,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dabe15b4-90ce-45bf-9f87-e3b3a03d5c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31367, 30, 30, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1faf96cd-f6c4-4c3a-8f57-b273131513a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31367,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea3e28-a9ea-4320-92fd-8fbdab5c5118",
   "metadata": {},
   "source": [
    "***converting to one hot encoding***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "701ac6c0-2590-4261-81d7-d659948e510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=to_categorical(y_train,43)\n",
    "y_test=to_categorical(y_test,43) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80e06f0-1d4b-457a-8be9-e506946ca4f3",
   "metadata": {},
   "source": [
    "***model building***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3aa4576-f2f4-44ea-be98-bf959fed2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41d5990a-732a-4700-8b27-07a68d41d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(filters=32,kernel_size=(5,5),activation='relu',input_shape=x_train.shape[1:]))\n",
    "model.add(Conv2D(filters=32,kernel_size=(5,5),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(43,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e66090b1-c813-4a52-b406-838dade0bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4219aa-904d-41ea-b2b5-e226086e2069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 53ms/step - accuracy: 0.2614 - loss: 3.7549 - val_accuracy: 0.8386 - val_loss: 0.7368\n",
      "Epoch 2/35\n",
      "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 58ms/step - accuracy: 0.7035 - loss: 1.0503 - val_accuracy: 0.9086 - val_loss: 0.3551\n",
      "Epoch 3/35\n",
      "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 58ms/step - accuracy: 0.7892 - loss: 0.7142 - val_accuracy: 0.9373 - val_loss: 0.2320\n",
      "Epoch 4/35\n",
      "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 60ms/step - accuracy: 0.8474 - loss: 0.5136 - val_accuracy: 0.9489 - val_loss: 0.1828\n",
      "Epoch 5/35\n",
      "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 59ms/step - accuracy: 0.8824 - loss: 0.4080 - val_accuracy: 0.9694 - val_loss: 0.1176\n",
      "Epoch 6/35\n",
      "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 54ms/step - accuracy: 0.8976 - loss: 0.3424 - val_accuracy: 0.9672 - val_loss: 0.1350\n",
      "Epoch 7/35\n",
      "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 55ms/step - accuracy: 0.9075 - loss: 0.3156 - val_accuracy: 0.9254 - val_loss: 0.2523\n",
      "Epoch 8/35\n",
      "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 54ms/step - accuracy: 0.9169 - loss: 0.2854 - val_accuracy: 0.9770 - val_loss: 0.0886\n",
      "Epoch 9/35\n",
      "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 55ms/step - accuracy: 0.9246 - loss: 0.2621 - val_accuracy: 0.9652 - val_loss: 0.1243\n",
      "Epoch 10/35\n",
      "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 56ms/step - accuracy: 0.9237 - loss: 0.2804 - val_accuracy: 0.9765 - val_loss: 0.0870\n",
      "Epoch 11/35\n",
      "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 57ms/step - accuracy: 0.9284 - loss: 0.2631 - val_accuracy: 0.9810 - val_loss: 0.0735\n",
      "Epoch 12/35\n",
      "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 56ms/step - accuracy: 0.9364 - loss: 0.2348 - val_accuracy: 0.9693 - val_loss: 0.1121\n",
      "Epoch 13/35\n",
      "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 56ms/step - accuracy: 0.9361 - loss: 0.2404 - val_accuracy: 0.9811 - val_loss: 0.0610\n",
      "Epoch 14/35\n",
      "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 56ms/step - accuracy: 0.9359 - loss: 0.2356 - val_accuracy: 0.9851 - val_loss: 0.0497\n",
      "Epoch 15/35\n",
      "\u001b[1m981/981\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 56ms/step - accuracy: 0.9400 - loss: 0.2183 - val_accuracy: 0.9827 - val_loss: 0.0589\n",
      "Epoch 16/35\n",
      "\u001b[1m348/981\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 52ms/step - accuracy: 0.9406 - loss: 0.2089"
     ]
    }
   ],
   "source": [
    "epochs=35\n",
    "history=model.fit(x_train,y_train,batch_size=32,epochs=epochs,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a72999-fd2e-4a64-a1c8-dd9412c6eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.plot(history.history['accuracy'],label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'],label='val accuracy')\n",
    "plt.title('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73645cf6-727f-405e-b27c-c050fa3912d0",
   "metadata": {},
   "source": [
    "***testing on data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e092c37-fb9c-4c9a-9b0b-0ce4949d5015",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09841253-8ca5-49f5-bbb5-7a229cc57893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def testing(testcsv):\n",
    "    y_test = pd.read_csv(testcsv) \n",
    "    label = y_test[\"ClassId\"].values \n",
    "    imgs = y_test[\"Path\"].values\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for img in imgs:\n",
    "        image = Image.open(img) \n",
    "        image = image.resize((30, 30))  \n",
    "        data.append(np.array(image))  \n",
    "    \n",
    "    x_test = np.array(data) \n",
    "    \n",
    "    return x_test, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb1f80-dae6-4028-acf2-d3f4dd0dbcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, labels = testing('Test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b5209-ecb5-4739-8651-85d7c75c5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6511f3-9683-4464-b974-5cbdbf4a8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032d462-7385-46a0-b183-01d5b1e5a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ee2c3-db81-4379-9142-719371aced89",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fbabdb-5902-420b-b70b-3c40ff9a5de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predd = model.predict(x_test)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873222d5-d65f-4734-9a60-949aaa349370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(labels,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16777e-d68e-4fa3-836e-20a1200f271a",
   "metadata": {},
   "source": [
    "***got the accuracy of 91%***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97412d1f-3b04-4bb5-9ee6-b0ab06c9bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = { 0:'Speed limit (20km/h)',\n",
    "            1:'Speed limit (30km/h)', \n",
    "            2:'Speed limit (50km/h)', \n",
    "            3:'Speed limit (60km/h)', \n",
    "            4:'Speed limit (70km/h)', \n",
    "            5:'Speed limit (80km/h)', \n",
    "            6:'End of speed limit (80km/h)', \n",
    "            7:'Speed limit (100km/h)', \n",
    "            8:'Speed limit (120km/h)', \n",
    "            9:'No passing', \n",
    "            10:'No passing veh over 3.5 tons', \n",
    "            11:'Right-of-way at intersection', \n",
    "            12:'Priority road', \n",
    "            13:'Yield', \n",
    "            14:'Stop', \n",
    "            15:'No vehicles', \n",
    "            16:'Veh > 3.5 tons prohibited', \n",
    "            17:'No entry', \n",
    "            18:'General caution', \n",
    "            19:'Dangerous curve left', \n",
    "            20:'Dangerous curve right', \n",
    "            21:'Double curve', \n",
    "            22:'Bumpy road', \n",
    "            23:'Slippery road', \n",
    "            24:'Road narrows on the right', \n",
    "            25:'Road work', \n",
    "            26:'Traffic signals', \n",
    "            27:'Pedestrians', \n",
    "            28:'Children crossing', \n",
    "            29:'Bicycles crossing', \n",
    "            30:'Beware of ice/snow',\n",
    "            31:'Wild animals crossing', \n",
    "            32:'End speed + passing limits', \n",
    "            33:'Turn right ahead', \n",
    "            34:'Turn left ahead', \n",
    "            35:'Ahead only', \n",
    "            36:'Go straight or right', \n",
    "            37:'Go straight or left', \n",
    "            38:'Keep right', \n",
    "            39:'Keep left', \n",
    "            40:'Roundabout mandatory', \n",
    "            41:'End of no passing', \n",
    "            42:'End no passing veh > 3.5 tons' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46176e80-8ea0-4d55-86dc-24064cfc0f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnnmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4fb15-5805-4839-a1d8-d1032a3f2e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebb50d9-99f7-4b22-9ee2-ef74ba3196c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = testing('Test.csv') \n",
    "class_names = [\n",
    "    'Speed limit (20km/h)', 'Speed limit (30km/h)', 'Speed limit (50km/h)', \n",
    "    'Speed limit (60km/h)', 'Speed limit (70km/h)', 'Speed limit (80km/h)', \n",
    "    'End of speed limit (80km/h)', 'Speed limit (100km/h)', 'Speed limit (120km/h)', \n",
    "    'No passing', 'No passing for vehicles over 3.5 metric tons', 'Right-of-way at the next intersection', \n",
    "    'Priority road', 'Yield', 'Stop', 'No vehicles', \n",
    "    'Vehicles over 3.5 metric tons prohibited', 'No entry', 'General caution', \n",
    "    'Dangerous curve to the left', 'Dangerous curve to the right', 'Double curve', \n",
    "    'Bumpy road', 'Slippery road', 'Road narrows on the right', \n",
    "    'Road work', 'Traffic signals', 'Pedestrians', \n",
    "    'Children crossing', 'Bicycles crossing', 'Beware of ice/snow', \n",
    "    'Wild animals crossing', 'End of all speed and passing limits', 'Turn right ahead', \n",
    "    'Turn left ahead', 'Ahead only', 'Go straight or right', \n",
    "    'Go straight or left', 'Keep right', 'Keep left', \n",
    "    'Roundabout mandatory', 'End of no passing', \n",
    "    'End of no passing by vehicles over 3.5 metric tons'\n",
    "]\n",
    "\n",
    "visualize_gtsrb_predictions(model, x_test, y_test, class_names, num_images=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d1709-1e40-4c26-8644-c8c1a85a8c50",
   "metadata": {},
   "source": [
    "The German Traffic Sign Recognition Benchmark (GTSRB) is a well-known dataset in the field of computer vision, specifically designed for traffic sign recognition. It contains a diverse collection of images representing 43 different classes of traffic signs, with a total of around 50,000 images.\n",
    "\n",
    "Objective:\n",
    "The main objective is to correctly classify each image into one of the 43 traffic sign categories. This problem is significant for developing advanced driver-assistance systems (ADAS) and autonomous vehicles.\n",
    "\n",
    "Dataset:\n",
    "Number of Classes: 43\n",
    "Number of Images: Approximately 50,000\n",
    "Image Characteristics: The images vary in resolution, lighting conditions, and angles, which presents a challenge for accurate classification.\n",
    "Methodology:\n",
    "i approached this problem using a Convolutional Neural Network (CNN), which is a deep learning model particularly effective for image recognition tasks. CNNs are well-suited for this problem due to their ability to capture spatial hierarchies in images through convolutional layers.\n",
    "\n",
    "CNN Architecture: Typically involves multiple layers, including convolutional layers, pooling layers, and fully connected layers, which together help in learning and identifying features like edges, textures, and more complex patterns relevant to traffic sign recognition.\n",
    "Training and Validation: The model was trained on a large portion of the dataset, with a smaller subset used for validation to tune the model's hyperparameters and prevent overfitting.\n",
    "Performance:\n",
    "Accuracy: You achieved an accuracy of 91%, which is a strong performance, indicating that your model can correctly classify traffic signs in the vast majority of cases.\n",
    "Implications:\n",
    "Achieving a high accuracy on the GTSRB dataset is a promising step toward developing systems that can accurately and reliably recognize traffic signs in real-world scenarios. With a 91% accuracy, your model is well-positioned to be used in applications that require real-time traffic sign recognition, though further improvements could be made to handle edge cases and enhance robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d0596-bb56-4d4e-862a-1ff8b8b4bafa",
   "metadata": {},
   "source": [
    "***trying yolo***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16710e38-f8e0-4926-aec9-8e03bf1d2254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c000d887-0090-42ed-862a-456dee799296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ff6f0-e481-4e63-b5a5-d057ffba1d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b14df7-f0eb-4fb0-a465-fb75eabf6239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
